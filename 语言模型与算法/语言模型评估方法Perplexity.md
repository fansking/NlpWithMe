# 语言模型评估方法Perplexity

如何评估一个语言模型的好坏呢？一个好的语言模型，对正常的句子和错误的句子的生成概率应该是有差异的。例如：老鼠爱吃大米    与     爱老鼠大米吃 。这两个句子分别放入语言模型中，第一个句子的生成概率应该较大。

困惑度（perplexity）的基本思想是：**给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好**

$PP(W)=P(w_{1}w_{2}...w_{N})^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_{1}w_{2}...w_{N})}}$

而根据n-gram模型， $P(w_{1}w_{2}...w_{N})$ 是可以算出来的（最大似然概率）

注意 这里$w_{1}w_{2}...w_{N}$ 是对句子分词得到的单词序列，N是单词总个数，由于加了负数的次方，故句子概率越大，语言模型越好，困惑度也即perplexity越小。