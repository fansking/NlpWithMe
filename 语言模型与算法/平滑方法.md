[toc]

# NLP语言模型平滑方法

在自然语言处理的模型中，我们需要输入一个句子，并给出这个句子出现的概率。我们通常会使用已有的语料库做先验概率统计，根据先验概率获得后验概率，但不幸的是，我们已有的语料库通常并不能覆盖所有的单词，当输入的句子中出现语料库中未曾包含的单词时，使用最大似然概率会导致这个句子出现的概率为0。这显然是不符合我们预期的，为了纠正这种错误，我们引入了平滑方法。

首先根据极大似然概率我们有以下公式:

$P_{MLE}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)}{c(w_i)}$

其中 $w_i$ 指出现在第i个位置的单词。  $c(w_i,...)$指$w_i...$单词序列出现的总次数

那么逐一介绍一些平滑方法

## Add-one

Add-one认为所有的单词序列都至少出现过一次，所以在分子会加一，分母会加所有的单词序列的个数。

这里仅以bi-gram举例,下式中V是词库的大小，因为在给定$w_{i-1}$的情况下，能出现的单词序列只有可能是词库所有单词遍历一遍也即$w_i$的可取值范围是整个词典。

$P_{Add-1}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)+1}{c(w_i)+V}$

那么为什么取这个V呢？实际上，无论如何进行平滑，都需要保证，概率总和相加为1。

## Add-K

Add-one看上去好像没什么毛病，为什么会有对Add-one的改进算法Add-K呢。事实上，当未出现过的单词序列较多时，会导致其占据的概率太大，使算法区分性不大，所以我们将1改为k，V改为kV。

$P_{Add-k}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)+k}{c(w_i)+kV}$

那么接踵而至的问题是如何选择k,一般而言认为k的值小于1，可以使用多次取值看效果的方法选定k，也有一种求函数极值的方法：

我们知道Perplexity(困惑度)是评价一个语言模型好坏的标准，困惑度根据语言模型给出的单词序列概率计算得出，而本方法得到的概率$P_{Add-k}$正是关于k的一个函数，那么得到的困惑度也必定是一个关于k的函数，当Perplexity取极小值时的k就是最优k取值。

## Interpolation

本方法提出是为了解决add-k方法中没有区分性的问题。举例如下：

c(chicken|like)=0

c(fish|like)=0

c(fish)=5

c(chicken)=0

那么使用add-k方法之后，p(fish|like)=p(chicken|like)=0.5 但实际上根据词典判断，p(fish|like)应该远大于p(chicken|like).对于这样的情况，我们需要从bi-gram考虑到ui-gram。所以其计算式如下:

$P_{int}(w_i|w_{i-1},w_{i-2})=\lambda_3P_{MLE}(w_i|w_{i-1},w_{i-2})+\lambda_2P_{MLE}(w_i|w_{i-1})+\lambda_1P_{MLE}(w_i)$

其中 $\lambda_3+\lambda_2+\lambda_1=1$

## Good-Turning

你可能会觉得，Interpolation方法几乎把所有能想到的情况都想到了，还能再改进吗？实际上还有一种情况上述方法没有考虑，当一个词(并不是单词序列)没有出现在词库中时，无论如何如何更改参数都无法使其概率不为0.

> 古德-图灵（Good-Turing）估计法是很多平滑技术的核心，于1953年有古德（I.J.Good）引用图灵（Turing）的方法而提出来的。其基本思想是：对于没有看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量（Probability Mass）中，分配一个很小的比例给这些没有看见的事件（如下图）。这样一来看的见概率总和就要小于1了，因此，需要将所有看见的事件概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。

假设语料库中出现r次的词有$N_r$个,语料库的大小为N。我们将概率方程式修改为如下公式:

对于没有出现过的单词:

$P_{GT}=\frac{N_1}{N}$

对于出现过的单词:

$P_{GT}=\frac{(c+1)N_{c+1}}{N_c}$

其中，c为该单词出现过的次数。

这种方法并非完美无缺，当r很大时，$N_r$可能为0，这就导致坟墓可能为0，为避免这些情况，我们可以对前r-1项进行曲线拟合，估计出$N_r$大小。