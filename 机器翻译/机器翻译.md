# 机器翻译

通常机器翻译可以分为两种，一种是通过词对词的一对一翻译，然后进行语法检查形成一个句子，这种方法被称为Statistical Machine Translation(基于统计模型的机器翻译)，另一种是通过深度学习完成。

## Statistical Machine Translation

以中译英作为举例：今晚的月色真美

1. 首先将中文句子进行分词划为token: 今晚  的  月色  真美

2. 将中文token翻译为英文token: Tonight of moonlight beautiful

3. 将英文token通过**排列组合**，得到$n!$个句子。例如：

   Tonight of moonlight beautiful

   Tonight of beautiful moonlight

   Tonight beautiful of moonlight

   Beautiful tonight of moonlight

   ......

4. 把得到的$n!$句子放入语言模型中，取语法正确概率最大的句子作为正确翻译输出。

### 翻译模型

翻译模型的使用在1,2,3中，翻译模型包括分词器和字典两部分。

结合下文的算法思想，其算法意义在于:

1. 给定一对<c,e>，计算p(c|e)
2. 语义相似度高，则p(c|e)高

### 语言模型

使用在4中，主要实现语法分析的功能

1. 给定一个英文e,计算概率p(e)
2. 如果符合英文语法，则p(e)高，如果是随机语句，p(e)则低。

### 核心算法思想

输入中文句子c ,想要得到一个英文句子e, 使得$P(e|c)$最大，也就是英文句子是中文句子正确翻译的结果的概率最大。

那么通过**贝叶斯定理**，我们可以知道 $P(e|c)=\frac{P(c|e)*P(e)}{P(c)}$,其中P(c)是定值，我们通常忽略他，可知$P(e|c)\propto P(c|e)*P(e)$。

那么问题转化成这两部分概率相乘，实际上，翻译模型解决的问题就是$P(c|e)$ :在英文是e的条件下，中文是c的概率,而语言模型解决的问题就是$P(e)$ ：英文e的语法正确概率。

#### 计算p(e)

通常使用N-gram模型。N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 Bi-gram (N=2) 和 Tri-gram (N=3)

1. $P(Tonight of moonlight beautiful)=P(Tonight)P(of)P(moonlight)P(beautiful)$
2. $P(Tonight of moonlight beautiful)=P(Tonight)P(of|Tonight)P(moonlight|of)P(beautiful|moonlight)$
3. $P(Tonight of moonlight beautiful)=P(Tonight)P(of|Tonight)P(moonlight|Tonight \quad of)P(beautiful|Tonight\quad of\quad moonlight)$

### 缺点

计算量大:全排列后判断，时间复杂度是$O(n!)$  （NP hard）

语义可能不符，或缺少部分词

## seq2seq

神经网络的机器翻译一般分为encoder和decoder两部分，其实相当于引入了中间语言类似java的JVM虚拟机。

具体细节待补充