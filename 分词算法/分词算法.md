[toc]

# 分词算法

中文分词算法主要分为**基于词表的分词算法、基于统计模型的分词算法、基于序列标注的分析算法。**

## 基于词表的分词算法

这一目录下的方法都是基于词表的分词算法，词表是指，包含所有词的表格。对于每一个子字符串，只有它出现在词表中，我们才认为它是一个合法的token。那么根据划分token时，指针移动的顺序，把这种方法又分为三类。指定maxLen是，考虑单词的最大长度，对于中文来讲，除了考虑专属名词，一般取5-6即可。作为示例，取maxLen为5

考虑句子: 我们经常有意见分歧

考虑词表：[我们，经常，有，有意见，意见，分歧]

### 正向最大匹配算法

其实很与滑动窗口很类似，设置最大窗口值并获取在窗口中的字符串，如果不在词表中，减小窗口大小，继续判断，直到判断出在此表中，或者只剩下一个字。那么使用剩下的字符串继续滑动窗口进行分词。

1. 考虑前五个字符:“我们经常有”。不在词表中。去除最后一个字符得到：“我们经常”，也不在词表中，继续去除，直到得到“我们”，在词表中，划分为token。
2. 考虑剩下的前五个字符: ”经常有意见“，步骤同1，得到token：“经常”
3. 重复上述操作，得到token为"有意见","分歧"

最终的分词结果为：我们/经常/有意见/分歧

### 反向最大匹配算法

与正向最大匹配算法类似，唯一的不同是，他是从左向右扫描。

1. 考虑后五个字符:“有意见分歧”。不在词表中。去除第一个字符得到：“意见分歧”，也不在词表中，继续去除，直到得到“分歧”，在词表中，划分为token。
2. 考虑剩下的后五个字符: ”经常有意见“，步骤同1，得到token：“有意见”
3. 重复上述操作，得到token为"经常","我们"

最终的分词结果为：我们/经常/有意见/分歧

你可能会说，这得到的结果怎么是一样的，确实这两种算法的大部分结果都是一样的，但也有不一样的。（应该能意会吧，我还真一下举不出例子）

### 双向最大匹配算法

双向最大匹配的意思就是，正向与反向都做一遍，取词数较少的作为结果，因为我们通常认为：**一个token包含字符越多，信息越大，正确性越高**

那么如果次数相同，就取单字最少的，若完全一样则返回任意一个

## 基于统计模型的分析算法



