[toc]

# 分词算法

中文分词算法主要分为**基于词表的分词算法、基于统计模型的分词算法、基于序列标注的分析算法。**

## 基于词表的分词算法

这一目录下的方法都是基于词表的分词算法，词表是指，包含所有词的表格。对于每一个子字符串，只有它出现在词表中，我们才认为它是一个合法的token。那么根据划分token时，指针移动的顺序，把这种方法又分为三类。指定maxLen是，考虑单词的最大长度，对于中文来讲，除了考虑专属名词，一般取5-6即可。作为示例，取maxLen为5

考虑句子: 我们经常有意见分歧

考虑词表：[我们，经常，有，有意见，意见，分歧]

### 正向最大匹配算法

其实很与滑动窗口很类似，设置最大窗口值并获取在窗口中的字符串，如果不在词表中，减小窗口大小，继续判断，直到判断出在此表中，或者只剩下一个字。那么使用剩下的字符串继续滑动窗口进行分词。

1. 考虑前五个字符:“我们经常有”。不在词表中。去除最后一个字符得到：“我们经常”，也不在词表中，继续去除，直到得到“我们”，在词表中，划分为token。
2. 考虑剩下的前五个字符: ”经常有意见“，步骤同1，得到token：“经常”
3. 重复上述操作，得到token为"有意见","分歧"

最终的分词结果为：我们/经常/有意见/分歧

### 反向最大匹配算法

与正向最大匹配算法类似，唯一的不同是，他是从左向右扫描。

1. 考虑后五个字符:“有意见分歧”。不在词表中。去除第一个字符得到：“意见分歧”，也不在词表中，继续去除，直到得到“分歧”，在词表中，划分为token。
2. 考虑剩下的后五个字符: ”经常有意见“，步骤同1，得到token：“有意见”
3. 重复上述操作，得到token为"经常","我们"

最终的分词结果为：我们/经常/有意见/分歧

你可能会说，这得到的结果怎么是一样的，确实这两种算法的大部分结果都是一样的，但也有不一样的。（应该能意会吧，我还真一下举不出例子）

### 双向最大匹配算法

双向最大匹配的意思就是，正向与反向都做一遍，取词数较少的作为结果，因为我们通常认为：**一个token包含字符越多，信息越大，正确性越高**

那么如果次数相同，就取单字最少的，若完全一样则返回任意一个

## 基于统计模型的分析算法

何为统计模型，统计通俗讲计数，实际上我们的N-gram模型就是典型的统计模型(真是哪里都躲不掉n-gram啊)。我们通过计算词频得到词语的出现概率，那么所有词语概率乘起来就是这个句子出现的概率（在这里可以理解为分词正确的概率）

实际上你可以发现似乎与上一篇文章机器翻译中的做法类似，都是算出所有的概率。只不过这里是对分词进行排列组合例如：

1. 我们/经常/有意见/分歧
2. 我们/经常/有/意见/分歧
3. 我们/经/常/有/意见/分歧
4. ......

### N-gram

把这些情况的概率全部算一遍，使用概率最大的分词结果就是最终结果。值得注意的是虽然bi-gram,tri-gram都是可以的，但是在分词中我们通常使用uni-gram，即不考虑依赖关系，因为分词可能会导致很多概率为0的词语。

### 维比特

维比特是一种动态规划算法，首先我们已经有每个分词的概率：

> 词典:["经常","经","有","有意见","意见","分歧","见","意","见分歧","分" ]
>
> 概率:[0.1,   0.05 , 0.1,   0.1,      0.2,        0.2,   0.05,  0.05 ,0.05  ,   0.1 ]
>
> -log(x):[2.3 , 3 ,     2.3,    2.3,     1.6  ,    1.6,    3,      3    ,     3      ,   2.3  ]

当然对于没有出现的词可以认为其概率很小比如$10^{-8}$

于是我们可以画出这样的图![维特比分词](D:\吕若凡\大三下\NlpWithMe\分词算法\维特比分词.png)

我们要找到一个路径，使得路径中-log(x)之和最小。现在转为一个最短路径问题，答案也比较明了。

如果做过《剑指offer》的同学应该都知道这是**剪绳子**问题，也就是逐步算出到第n个节点最好的走法就可以了。

## 基于序列标注的分词算法

### HMM

### MEMM

### CRF

